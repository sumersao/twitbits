{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertModel, LEDForConditionalGeneration\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our augmented dataset json file\n",
    "augmented_dataset = None\n",
    "with open('augmented_test.json', 'r') as f:\n",
    "    augmented_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(documents):\n",
    "    doc_embeddings = []\n",
    "    for doc in documents:\n",
    "        inputs = tokenizer.encode_plus(doc, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        # we're going to average across the tokens to get the sentence embedding\n",
    "        doc_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "        doc_embeddings.append(doc_embedding)\n",
    "    \n",
    "    return doc_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_documents_kmeans(documents):\n",
    "    doc_embeddings = embed_documents(documents)\n",
    "    # turn our list of embeddings into a numpy matrix\n",
    "    doc_embeddings = torch.cat(doc_embeddings).detach().numpy()\n",
    "    # cluster them into 3 clusters\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0).fit(doc_embeddings)\n",
    "    return kmeans.labels_\n",
    "\n",
    "def cluster_documents_hierarchical(documents):\n",
    "    doc_embeddings = embed_documents(documents)\n",
    "    # turn our list of embeddings into a numpy matrix\n",
    "    doc_embeddings = torch.cat(doc_embeddings).detach().numpy()\n",
    "    # cluster them into 3 clusters\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=3).fit(doc_embeddings)\n",
    "    return hierarchical.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our model\n",
    "TOKENIZER = AutoTokenizer.from_pretrained('allenai/PRIMERA')\n",
    "MODEL = LEDForConditionalGeneration.from_pretrained('allenai/PRIMERA').to(device)\n",
    "\n",
    "PAD_TOKEN_ID = TOKENIZER.pad_token_id\n",
    "DOCSEP_TOKEN_ID = TOKENIZER.convert_tokens_to_ids(\"<doc-sep>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our indices to use\n",
    "sample_indices = pd.read_csv('cohere_sample_indices.csv').values.flatten()\n",
    "dataset_small = [augmented_dataset[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(documents):\n",
    "    input_ids_all=[]\n",
    "    #### concat with global attention on doc-sep\n",
    "    input_ids = []\n",
    "    for doc in documents:\n",
    "        input_ids.extend(\n",
    "            TOKENIZER.encode(\n",
    "                doc,\n",
    "                truncation=True,\n",
    "                max_length=4096 // len(documents),\n",
    "            )[1:-1]\n",
    "        )\n",
    "        input_ids.append(DOCSEP_TOKEN_ID)\n",
    "    input_ids = (\n",
    "        [TOKENIZER.bos_token_id]\n",
    "        + input_ids\n",
    "        + [TOKENIZER.eos_token_id]\n",
    "    )\n",
    "    input_ids_all.append(torch.tensor(input_ids).to(device))\n",
    "    input_ids_final = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids_all, batch_first=True, padding_value=PAD_TOKEN_ID\n",
    "    ).to(device)\n",
    "\n",
    "    return input_ids_final\n",
    "\n",
    "\"\"\"\n",
    "For each cluster, use each document in input. Output 512/3 tokens for each cluster. Append them. \n",
    "\"\"\"\n",
    "def process_clusters_concat(data, clusters):\n",
    "    documents = data['documents']\n",
    "    # create a list of lists, where each list is a cluster\n",
    "    cluster_docs = [[] for _ in range(3)]\n",
    "    for i, doc in enumerate(documents):\n",
    "        cluster_docs[clusters[i]].append(doc)\n",
    "    cluster_summaries = []\n",
    "    \n",
    "    # now run the summarization on each cluster, use the first document``\n",
    "    for cluster in cluster_docs:\n",
    "        input_ids = process_document(cluster)\n",
    "        # get the input ids and attention masks together\n",
    "        global_attention_mask = torch.zeros_like(input_ids).to(input_ids.device)\n",
    "        # put global attention on <s> token\n",
    "\n",
    "        global_attention_mask[:, 0] = 1\n",
    "        global_attention_mask[input_ids == DOCSEP_TOKEN_ID] = 1\n",
    "\n",
    "        generated_ids = MODEL.generate(\n",
    "            input_ids=input_ids,\n",
    "            global_attention_mask=global_attention_mask,\n",
    "            use_cache=True,\n",
    "            max_length=512//3,\n",
    "            num_beams=5,\n",
    "        )\n",
    "\n",
    "        generated_str = TOKENIZER.batch_decode(\n",
    "                generated_ids.tolist(), skip_special_tokens=True\n",
    "            )\n",
    "        cluster_summaries.append(generated_str)\n",
    "    \n",
    "    result={}\n",
    "    # concatenate the summaries\n",
    "    result['generated_summaries'] = [' '.join(cluster) for cluster in cluster_summaries]\n",
    "    result['gt_summaries']= data['summary']\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "Take 1 random document from each cluster and just use that to summarize (idea â€” get more context on \n",
    "each document). Output max length still 512 tokens. \n",
    "\"\"\"\n",
    "def process_clusters_one_doc(data, clusters):\n",
    "    documents = data['documents']\n",
    "    # create a list of lists, where each list is a cluster\n",
    "    cluster_docs = [[] for _ in range(3)]\n",
    "    for i, doc in enumerate(documents):\n",
    "        cluster_docs[clusters[i]].append(doc)\n",
    "\n",
    "    new_docs = [cluster[0] for cluster in cluster_docs]\n",
    "    input_ids = process_document(new_docs)\n",
    "    \n",
    "    # get the input ids and attention masks together\n",
    "    global_attention_mask = torch.zeros_like(input_ids).to(input_ids.device)\n",
    "    # put global attention on <s> token\n",
    "\n",
    "    global_attention_mask[:, 0] = 1\n",
    "    global_attention_mask[input_ids == DOCSEP_TOKEN_ID] = 1\n",
    "\n",
    "    generated_ids = MODEL.generate(\n",
    "        input_ids=input_ids,\n",
    "        global_attention_mask=global_attention_mask,\n",
    "        use_cache=True,\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "    )\n",
    "\n",
    "    generated_str = TOKENIZER.batch_decode(\n",
    "            generated_ids.tolist(), skip_special_tokens=True\n",
    "        )\n",
    "    \n",
    "    result={}\n",
    "    result['generated_summaries'] = generated_str\n",
    "    result['gt_summaries']= data['summary']\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our pipeline using kmeans clustering\n",
    "\n",
    "result_small_one_doc = []\n",
    "result_small_concat = []\n",
    "\n",
    "for i in tqdm(range(0, len(dataset_small))):\n",
    "    data = dataset_small[i]\n",
    "    k_clusters = cluster_documents_kmeans(data['documents'])\n",
    "\n",
    "    result_small_one_doc.append(process_clusters_one_doc(data, k_clusters))\n",
    "    result_small_concat.append(process_clusters_concat(data, k_clusters))\n",
    "\n",
    "    # let's save the result every 50 points\n",
    "    if i % 50 == 0:\n",
    "        with open(f\"results/kmeans_bert_one_doc{i}.json\", \"w\") as f:\n",
    "            json.dump(result_small_one_doc, f)\n",
    "        \n",
    "        with open(f\"results/kmeans_bert_concat{i}.json\", \"w\") as f:\n",
    "            json.dump(result_small_concat, f)\n",
    "        \n",
    "\n",
    "with open(f\"results/kmeans_bert_one_doc_complete.json\", \"w\") as f:\n",
    "    json.dump(result_small_one_doc, f)\n",
    "\n",
    "with open(f\"results/kmeans_bert_concat_complete.json\", \"w\") as f:\n",
    "    json.dump(result_small_concat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summaries_one_doc_kmeans = [result['generated_summaries'] for result in result_small_one_doc]\n",
    "generated_summaries_concat_kmeans = [result['generated_summaries'] for result in result_small_concat]\n",
    "gt_summaries = [result['gt_summaries'] for result in result_small_one_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our pipeline using hierarchical clustering\n",
    "result_small_one_doc = []\n",
    "result_small_concat = []\n",
    "\n",
    "for i in tqdm(range(0, len(dataset_small))):\n",
    "    data = dataset_small[i]\n",
    "    k_clusters = cluster_documents_hierarchical(data['documents'])\n",
    "\n",
    "    result_small_one_doc.append(process_clusters_one_doc(data, k_clusters))\n",
    "    result_small_concat.append(process_clusters_concat(data, k_clusters))\n",
    "\n",
    "    # let's save the result every 50 points\n",
    "    if i % 50 == 0:\n",
    "        with open(f\"results/hierarchical_bert_one_doc{i}.json\", \"w\") as f:\n",
    "            json.dump(result_small_one_doc, f)\n",
    "        \n",
    "        with open(f\"results/hierarchical_bert_concat{i}.json\", \"w\") as f:\n",
    "            json.dump(result_small_concat, f)\n",
    "        \n",
    "\n",
    "with open(f\"results/hierarchical_bert_one_doc_complete.json\", \"w\") as f:\n",
    "    json.dump(result_small_one_doc, f)\n",
    "\n",
    "with open(f\"results/hierarchical_bert_concat_complete.json\", \"w\") as f:\n",
    "    json.dump(result_small_concat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summaries_one_doc_hier = [result['generated_summaries'] for result in result_small_one_doc]\n",
    "generated_summaries_concat_hier = [result['generated_summaries'] for result in result_small_concat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m0/l6jpq4w579906wrx6f1v04700000gn/T/ipykernel_36390/4132584981.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=rouge.compute(predictions=generated_summaries_one_doc_kmeans, references=gt_summaries)\n",
    "print(score['rouge1'].mid)\n",
    "print(score['rouge2'].mid)\n",
    "print(score['rougeL'].mid)\n",
    "\n",
    "score=rouge.compute(predictions=generated_summaries_concat_kmeans, references=gt_summaries)\n",
    "print(score['rouge1'].mid)\n",
    "print(score['rouge2'].mid)\n",
    "print(score['rougeL'].mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=rouge.compute(predictions=generated_summaries_one_doc_hier, references=gt_summaries)\n",
    "print(score['rouge1'].mid)\n",
    "print(score['rouge2'].mid)\n",
    "print(score['rougeL'].mid)\n",
    "\n",
    "score=rouge.compute(predictions=generated_summaries_concat_hier, references=gt_summaries)\n",
    "print(score['rouge1'].mid)\n",
    "print(score['rouge2'].mid)\n",
    "print(score['rougeL'].mid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitbits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
