{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertModel, LEDForConditionalGeneration\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, pairwise_distances_argmin_min\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our augmented dataset json file\n",
    "augmented_dataset = None\n",
    "with open('augmented_test.json', 'r') as f:\n",
    "    augmented_dataset = json.load(f)\n",
    "\n",
    "# load our indices to use\n",
    "sample_indices = pd.read_csv('cohere_sample_indices.csv').values.flatten()\n",
    "dataset_small = [augmented_dataset[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load our model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(documents):\n",
    "    doc_embeddings = []\n",
    "    for doc in documents:\n",
    "        inputs = tokenizer.encode_plus(doc, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        # we're going to average across the tokens to get the sentence embedding\n",
    "        doc_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "        doc_embeddings.append(doc_embedding)\n",
    "\n",
    "    return doc_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters_kmeans(documents):\n",
    "    doc_embeddings = embed_documents(documents)\n",
    "    # turn our list of embeddings into a numpy matrix\n",
    "    doc_embeddings = torch.cat(doc_embeddings).detach().numpy()\n",
    "\n",
    "    # we are going to fine tune the number of clusters to use, based on silhoutte score\n",
    "    best_score = -2\n",
    "    best_k = 0\n",
    "    score_k_3 = -2\n",
    "    for k in range(2, min(10, len(doc_embeddings))):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0, n_init=10).fit(doc_embeddings)\n",
    "        ss = silhouette_score(doc_embeddings, kmeans.labels_)\n",
    "        if ss > best_score:\n",
    "            best_score = ss\n",
    "            best_k = k\n",
    "        if k == 3:\n",
    "            score_k_3 = ss\n",
    "        \n",
    "    return best_k, best_score, score_k_3\n",
    "\n",
    "num_docs = []\n",
    "ks = []\n",
    "scores = []\n",
    "scores_k_3 = []\n",
    "\n",
    "for i in tqdm(range(0, len(dataset_small))):\n",
    "    data = dataset_small[i]\n",
    "\n",
    "    best_k, best_score, score_k_3 = analyze_clusters_kmeans(data['documents'])\n",
    "    num_docs.append(len(data['documents']))\n",
    "    ks.append(best_k)\n",
    "    scores.append(best_score)\n",
    "    scores_k_3.append(score_k_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plots directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists('plots'):\n",
    "    os.mkdir('plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to plot 2 bar charts. Comparing the average silhoutte scores for the number\n",
    "# of documents, and the average k value for the number of documents\n",
    "\n",
    "# first, we need to get the average silhoutte scores for each number of documents\n",
    "num_docs_to_scores = {}\n",
    "num_docs_to_ks = {}\n",
    "\n",
    "for i in range(0, len(num_docs)):\n",
    "    num_docs_to_scores[num_docs[i]] = num_docs_to_scores.get(num_docs[i], []) + [scores[i]]\n",
    "    num_docs_to_ks[num_docs[i]] = num_docs_to_ks.get(num_docs[i], []) + [ks[i]]\n",
    "\n",
    "num_docs_to_avg_scores = {}\n",
    "num_docs_to_avg_ks = {}\n",
    "for num_doc, cur_scores in num_docs_to_scores.items():\n",
    "    num_docs_to_avg_scores[num_doc] = sum(cur_scores) / len(cur_scores)\n",
    "    num_docs_to_avg_ks[num_doc] = sum(num_docs_to_ks[num_doc]) / len(num_docs_to_ks[num_doc])\n",
    "\n",
    "\n",
    "# now we can plot the bar charts\n",
    "if not os.path.exists('plots/kmeans'):\n",
    "    os.mkdir('plots/kmeans')\n",
    "\n",
    "\n",
    "plt.bar(num_docs_to_avg_scores.keys(), num_docs_to_avg_scores.values())\n",
    "plt.title('Average Silhoutte Score for Number of Documents')\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Average Silhoutte Score')\n",
    "plt.savefig('plots/kmeans/avg_silhoutte_score_num_docs.png')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(num_docs_to_avg_ks.keys(), num_docs_to_avg_ks.values())\n",
    "plt.title('Average K Value for Number of Documents')\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Average K Value')\n",
    "plt.savefig('plots/kmeans/avg_k_value_num_docs.png')\n",
    "plt.show()\n",
    "\n",
    "# next plot the distribution of documents\n",
    "plt.hist(num_docs)\n",
    "plt.title('Distribution of Number of Documents')\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('plots/kmeans/distribution_num_docs.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "with open('plots/kmeans/analysis.txt', 'w') as f:\n",
    "    f.write(f\"Avg silhoutte: {sum(scores) / len(scores)}\\n\")\n",
    "    f.write(f\"Avg silhoutte k=3: {sum(scores_k_3) / len(scores_k_3)}\\n\")\n",
    "\n",
    "    f.write(\"Num docs, q1, q2, q3, min, max, var\\n\")\n",
    "    \n",
    "    # also compute the quartiles for the distribution of k values for each number of documents\n",
    "    for num_doc, ks in num_docs_to_ks.items():\n",
    "        # compute the quartiles of ks and the min and max\n",
    "        q1 = np.quantile(ks, 0.25)\n",
    "        q2 = np.quantile(ks, 0.5)\n",
    "        q3 = np.quantile(ks, 0.75)\n",
    "        min_k = min(ks)\n",
    "        max_k = max(ks)\n",
    "        # compute the variance of ks\n",
    "        var = np.var(ks)\n",
    "        f.write(f\"{num_doc}: {q1}, {q2}, {q3}, {min_k}, {max_k}, {var} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters_kmeans_pca(documents):\n",
    "    doc_embeddings = embed_documents(documents)\n",
    "    # turn our list of embeddings into a numpy matrix\n",
    "    doc_embeddings = torch.cat(doc_embeddings).detach().numpy()\n",
    "    \n",
    "    # run PCA on the embeddings first\n",
    "    pca = PCA(n_components=0.99)\n",
    "    pca.fit(doc_embeddings)\n",
    "    doc_embeddings = pca.fit_transform(doc_embeddings)\n",
    "\n",
    "    # we are going to fine tune the number of clusters to use, based on silhoutte score\n",
    "    best_score = -2\n",
    "    best_k = 0\n",
    "    score_k_3 = -2\n",
    "\n",
    "    for k in range(2, min(10, len(doc_embeddings))):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0, n_init=10).fit(doc_embeddings)\n",
    "        ss = silhouette_score(doc_embeddings, kmeans.labels_)\n",
    "        if ss > best_score:\n",
    "            best_score = ss\n",
    "            best_k = k\n",
    "        if k == 3:\n",
    "            score_k_3 = ss\n",
    "        \n",
    "    return best_k, best_score, score_k_3\n",
    "\n",
    "num_docs = []\n",
    "ks = []\n",
    "scores = []\n",
    "scores_k_3 = []\n",
    "\n",
    "for i in tqdm(range(0, len(dataset_small))):\n",
    "    data = dataset_small[i]\n",
    "\n",
    "    best_k, best_score, score_k_3 = analyze_clusters_kmeans_pca(data['documents'])\n",
    "\n",
    "    num_docs.append(len(data['documents']))\n",
    "    ks.append(best_k)\n",
    "    scores.append(best_score)\n",
    "    scores_k_3.append(score_k_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to plot 2 bar charts. Comparing the average silhoutte scores for the number\n",
    "# of documents, and the average k value for the number of documents\n",
    "\n",
    "# first, we need to get the average silhoutte scores for each number of documents\n",
    "num_docs_to_scores = {}\n",
    "num_docs_to_ks = {}\n",
    "\n",
    "for i in range(0, len(num_docs)):\n",
    "    num_docs_to_scores[num_docs[i]] = num_docs_to_scores.get(num_docs[i], []) + [scores[i]]\n",
    "    num_docs_to_ks[num_docs[i]] = num_docs_to_ks.get(num_docs[i], []) + [ks[i]]\n",
    "\n",
    "num_docs_to_avg_scores = {}\n",
    "num_docs_to_avg_ks = {}\n",
    "for num_doc, cur_scores in num_docs_to_scores.items():\n",
    "    num_docs_to_avg_scores[num_doc] = sum(cur_scores) / len(cur_scores)\n",
    "    num_docs_to_avg_ks[num_doc] = sum(num_docs_to_ks[num_doc]) / len(num_docs_to_ks[num_doc])\n",
    "\n",
    "\n",
    "# now we can plot the bar charts\n",
    "if not os.path.exists('plots/kmeans_pca'):\n",
    "    os.mkdir('plots/kmeans_pca')\n",
    "\n",
    "\n",
    "plt.bar(num_docs_to_avg_scores.keys(), num_docs_to_avg_scores.values())\n",
    "plt.title('Average Silhoutte Score for Number of Documents')\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Average Silhoutte Score')\n",
    "plt.savefig('plots/kmeans_pca/avg_silhoutte_score_num_docs.png')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(num_docs_to_avg_ks.keys(), num_docs_to_avg_ks.values())\n",
    "plt.title('Average K Value for Number of Documents')\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Average K Value')\n",
    "plt.savefig('plots/kmeans_pca/avg_k_value_num_docs.png')\n",
    "plt.show()\n",
    "\n",
    "# next plot the distribution of documents\n",
    "plt.hist(num_docs)\n",
    "plt.title('Distribution of Number of Documents')\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('plots/kmeans_pca/distribution_num_docs.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "with open('plots/kmeans_pca/analysis.txt', 'w') as f:\n",
    "    f.write(f\"Avg silhoutte: {sum(scores) / len(scores)}\\n\")\n",
    "    f.write(f\"Avg silhoutte k=3: {sum(scores_k_3) / len(scores_k_3)}\\n\")\n",
    "\n",
    "    f.write(\"Num docs, q1, q2, q3, min, max, var\\n\")\n",
    "    \n",
    "    # also compute the quartiles for the distribution of k values for each number of documents\n",
    "    for num_doc, ks in num_docs_to_ks.items():\n",
    "        # compute the quartiles of ks and the min and max\n",
    "        q1 = np.quantile(ks, 0.25)\n",
    "        q2 = np.quantile(ks, 0.5)\n",
    "        q3 = np.quantile(ks, 0.75)\n",
    "        min_k = min(ks)\n",
    "        max_k = max(ks)\n",
    "        # compute the variance of ks\n",
    "        var = np.var(ks)\n",
    "        f.write(f\"{num_doc}: {q1}, {q2}, {q3}, {min_k}, {max_k}, {var} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, doc_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitbits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
